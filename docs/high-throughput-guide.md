# High-throughput guide (8K+ messages/second)

This document covers performance considerations and optimization strategies when
handling thousands of MQTT messages per second in a desktop UI application.

## The challenge

MQTT brokers can deliver messages far faster than a UI framework can render them.
At 8K messages/second, the app receives a new message every 0.125 milliseconds.
Meanwhile, a typical UI renders at 60 FPS (16.6 ms per frame). This mismatch
requires careful buffering and throttling.

## UI thread dispatcher queue

Avalonia (like WPF) uses a dispatcher queue to marshal work to the UI thread. Every
`Dispatcher.UIThread.Post()` call adds an item to this queue. Problems arise when:

- The queue grows faster than the UI thread can process it.
- Even low-priority items must be tracked and eventually processed.
- Memory accumulates for queued delegates, captured variables, and operation wrappers.
- Large queues delay garbage collection of captured objects.

### How the dispatcher processes priorities

**Critical: Input Starvation Prevention**

The dispatcher includes built-in protection to keep the UI responsive. When processing items at or below **Input** priority (-1), it checks:

1. **Pending input**: Does the OS have pending keyboard/mouse events?
2. **Time limit**: Has it been > 1 second since last input processing?

If either condition is true, the dispatcher **stops processing Background priority items** and waits for the next frame. This prevents low-priority work from blocking user interaction.

**Important**: Priorities **above Input** (Render=4, Normal=8, etc.) always execute immediately and bypass these checks.

```mermaid
flowchart TB
    subgraph producers["âš¡ Producer Threads"]
        direction LR
        P1["MQTT<br/>Callback"]
        P2["User<br/>Input"]
        P3["Timer<br/>Event"]
    end

    subgraph queue["ğŸ“‹ Dispatcher Priority Queues"]
        direction TB
        Q1["ğŸ”´ Send (9)"]
        Q2["ğŸŸ  Normal (8)"]
        Q3["ğŸŸ¡ Render (4)"]
        Q4["ğŸŸ¢ Default (0)"]
        Q5["ğŸŸ£ Input (-1)"]
        Q6["ğŸŸ¤ Background (-2)"]
        Q7["âšª SystemIdle (-5)"]
    end

    subgraph processing["ğŸ”„ ExecuteJobsCore Loop"]
        direction TB
        PEEK["1ï¸âƒ£ Peek max priority<br/>(O(1) from SortedList)"]
        CHECK_ITEMS{"2ï¸âƒ£ Queue<br/>empty?"}
        CHECK_PRIORITY{"3ï¸âƒ£ Priority<br/>> Input (-1)?"}
        CHECK_TIME{"4ï¸âƒ£ Time since input<br/>> 1 second?"}
        CHECK_INPUT{"5ï¸âƒ£ OS has<br/>pending input?"}
        EXEC["6ï¸âƒ£ Dequeue & Execute"]
        DEFER["â¸ï¸ Stop processing<br/>(wait for next frame)"]
        DONE["âœ… Done<br/>(wait for signal)"]
        
        PEEK --> CHECK_ITEMS
        CHECK_ITEMS -->|"Yes"| DONE
        CHECK_ITEMS -->|"No"| CHECK_PRIORITY
        CHECK_PRIORITY -->|"Yes<br/>(Render, Normal, etc.)"| EXEC
        CHECK_PRIORITY -->|"No<br/>(Input, Background, etc.)"| CHECK_TIME
        CHECK_TIME -->|"Yes"| DEFER
        CHECK_TIME -->|"No"| CHECK_INPUT
        CHECK_INPUT -->|"Yes"| DEFER
        CHECK_INPUT -->|"No"| EXEC
        EXEC --> PEEK
        DEFER --> DONE
        DONE --> PEEK
    end

    P1 -.->|"Post(Background)"| Q6
    P2 -.->|"Post(Input)"| Q5
    P3 -.->|"Post(Render)"| Q3
    
    queue ==> PEEK
    
    style CHECK_PRIORITY fill:#ffe6e6
    style CHECK_TIME fill:#fff3cd
    style CHECK_INPUT fill:#fff3cd
    style DEFER fill:#f8d7da
```

**Key behaviors verified in source:**

- **ExecuteJobsCore** is the main processing loop in [Dispatcher.Queue.cs](https://raw.githubusercontent.com/AvaloniaUI/Avalonia/master/src/Avalonia.Base/Threading/Dispatcher.Queue.cs)
- `if (job.Priority > DispatcherPriority.Input)` determines if starvation checks apply (line 156)
- `MaximumInputStarvationTime` defaults to 1 second, configurable via [DispatcherOptions.InputStarvationTimeout](https://raw.githubusercontent.com/AvaloniaUI/Avalonia/master/src/Avalonia.Base/Threading/DispatcherOptions.cs#L18)
- Platform differences:
  * Modern: `IDispatcherImplWithExplicitBackgroundProcessing` signals background work via `ReadyForBackgroundProcessing` event
  * Legacy: `LegacyDispatcherImpl` falls back to time-based checking
  * Desktop platforms: `IDispatcherImplWithPendingInput.HasPendingInput` checks for OS input queue

**Real-world impact with active user input:**
- Background items process in 10-100ms bursts between user interactions
- Effective rate: ~30-120 background items/second (not thousands)
- Queue growth: At 8K msg/s, ~7,900 items accumulate per second
- Memory: ~1.2 MB/second for operation wrappers alone

### The queue growth problem at scale

```mermaid
flowchart TB
    subgraph normal["âšª Normal State: 15 items"]
        direction LR
        N1["ğŸ’¾ 10 KB"]
        N2["âš¡ Responsive"]
        N3["ğŸ—‘ï¸ Gen0 GC"]
    end
    
    ARROW1["â¬‡ï¸ High throughput (8K msg/s Ã— 5s)"]
    
    subgraph load["âš™ï¸ Processing"]
        direction LR
        L1["ğŸ“¨ 40,000 Post() calls"]
        L2["â¸ï¸ Zero delay"]
        L3["â• No drops"]
    end
    
    ARROW2["â¬‡ï¸ Queue growth"]
    
    subgraph overload["ğŸ”´ Overload State: 40K items"]
        direction LR
        O1["ğŸ’¾ 10+ MB memory"]
        O2["ğŸŒ UI Frozen"]
        O3["ğŸ—‘ï¸ Gen2 GC"]
    end
    
    normal --> ARROW1
    ARROW1 --> load
    load --> ARROW2
    ARROW2 --> overload
    
    subgraph cost["ğŸ’° Memory Cost per Item"]
        direction TB
        C1["DispatcherOperation: ~100 B"]
        C2["Delegate object: ~50 B"]
        C3["Captured closure: ~50-150 B"]
        C4["â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"]
        C5["Total: ~200-300 B"]
        C6[" "]
        C7["40,000 items Ã— 250 B = 10 MB"]
    end
    
    overload -.->|"Breakdown"| cost
```

When low-priority queues grow to tens of thousands of items:
- **Memory overhead**: Each `Post()` allocates a `DispatcherOperation` wrapper (~100 bytes) plus delegate and captured state
- **GC pressure**: Large queues can promote objects to Gen1/Gen2, causing expensive full collections
- **Delayed finalization**: Objects captured by closures can't be collected until the operation executes
- **Throughput impact**: While queue operations are O(1), processing 40K items takes time even at 60 FPS

### Queue accumulation vs. processing rate

**Priority bucket separation: How dispatcher avoids scanning**

The dispatcher uses **separate buckets** for each priority (via `SortedList<int, PriorityChain>`). This means:
- âœ… O(1) lookup to find highest priority bucket (no scanning!)
- âœ… Click events picked immediately, even with 5000 MQTT items queued
- âœ… Priorities don't compete during selection

**Real-world scenario with mixed events:**

```mermaid
flowchart TB
    subgraph events["Events Posted to Dispatcher"]
        e1["1ï¸âƒ£ Click"]
        e2["2ï¸âƒ£ Draw UI"]
        e3["3ï¸âƒ£ Draw UI"]
        e4["4ï¸âƒ£ Mouse Move"]
        e5["5ï¸âƒ£ Draw UI"]
        e6["6ï¸âƒ£ MQTTâ†’200 ..."]
        e7["201ï¸âƒ£ Click"]
        e8["202ï¸âƒ£ MQTTâ†’5000 ..."]
        e9["5001ï¸âƒ£ Click"]
    end
    
    subgraph buckets["Dispatcher Priority Buckets (Separate Chains)"]
        subgraph render["ğŸ”´ Render Priority (value: 4)"]
            r1["2. Draw UI"]
            r2["3. Draw UI"]
            r3["5. Draw UI"]
        end
        
        subgraph input["ğŸŸ¡ Input Priority (value: -1)"]
            i1["1. Click"]
            i2["4. Mouse Move"]
            i3["201. Click"]
            i4["5001. Click"]
        end
        
        subgraph background["ğŸŸ¢ Background Priority (value: -2)"]
            b1["6. MQTT Update"]
            b2["7. MQTT Update"]
            b3["..."]
            b4["200. MQTT Update"]
            b5["202. MQTT Update"]
            b6["..."]
            b7["5000. MQTT Update"]
        end
    end
    
    subgraph dispatch["Dispatcher Scan Logic"]
        scan["ğŸ” Get Max Priority<br/>O(1) lookup in<br/>SortedList.Keys"]
        check["Which bucket<br/>has items?"]
        pick["Pick from head<br/>of that chain"]
    end
    
    e2 & e3 & e5 -->|"Render priority"| render
    e1 & e4 & e7 & e9 -->|"Input priority"| input
    e6 & e8 -->|"Background priority"| background
    
    render -->|"Highest priority (4)"| scan
    input -->|"Middle priority (-1)"| scan
    background -->|"Lowest priority (-2)"| scan
    
    scan --> check
    check -->|"1. Check Render first"| pick
    
    style render fill:#ffe6e6
    style input fill:#fff9e6
    style background fill:#e6f7e6
    style scan fill:#e6f0ff
```

**Detailed timeline showing priority selection:**

```
Event   â”‚ Action                           â”‚ Buckets After Post
â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
1ï¸âƒ£      â”‚ Post(Click, Input)               â”‚ Input: [1]
2ï¸âƒ£      â”‚ Post(Draw, Render)               â”‚ Render: [2], Input: [1]
3ï¸âƒ£      â”‚ Post(Draw, Render)               â”‚ Render: [2,3], Input: [1]
4ï¸âƒ£      â”‚ Post(Mouse, Input)               â”‚ Render: [2,3], Input: [1,4]
5ï¸âƒ£      â”‚ Post(Draw, Render)               â”‚ Render: [2,3,5], Input: [1,4]
6-200   â”‚ Post(MQTTÃ—195, Background)       â”‚ Render: [2,3,5], Input: [1,4], 
        â”‚                                  â”‚ Background: [6-200]
â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        â”‚ ğŸ” Dispatcher Frame #1 (16ms)
        â”‚ scan.GetMaxPriority() â†’ 4 (Render)
        â”‚ âœ… Pick item #2 from Render      â”‚ Render: [3,5], Input: [1,4],
        â”‚    (skips 195 MQTT items!)       â”‚ Background: [6-200]
â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        â”‚ ğŸ” Dispatcher Frame #2 (33ms)
        â”‚ scan.GetMaxPriority() â†’ 4 (Render)
        â”‚ âœ… Pick item #2 from Render      â”‚ Render: [3,5], Input: [1,4],
        â”‚                                  â”‚ Background: [6-200]
â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        â”‚ ğŸ” Dispatcher Frame #2 (33ms)
        â”‚ scan.GetMaxPriority() â†’ 4 (Render)
        â”‚ âœ… Pick item #5 from Render      â”‚ Render: [], Input: [1,4],
        â”‚                                  â”‚ Background: [6-200]
â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        â”‚ ğŸ” Dispatcher Frame #4 (66ms)
        â”‚ scan.GetMaxPriority() â†’ -1 (Input)
        â”‚ âœ… Pick item #1 from Input       â”‚ Render: [], Input: [4],
        â”‚    (still skips 195 MQTT!)       â”‚ Background: [6-200]
â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        â”‚ ğŸ” Dispatcher Frame #5 (83ms)
        â”‚ scan.GetMaxPriority() â†’ -1 (Input)
        â”‚ âœ… Pick item #4 from Input       â”‚ Render: [], Input: [],
        â”‚                                  â”‚ Background: [6-200]
â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        â”‚ ğŸ” Dispatcher Frame #6 (100ms)
        â”‚ scan.GetMaxPriority() â†’ -2 (Background)
        â”‚ âœ… Pick item #6 from Background  â”‚ Render: [], Input: [],
        â”‚    NOW MQTT items start processingâ”‚ Background: [7-200]
â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
201ï¸âƒ£    â”‚ Post(Click, Input) âš¡ HIGH PRIORITYâ”‚ Render: [], Input: [201],
        â”‚                                  â”‚ Background: [7-200]
â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        â”‚ ğŸ” Dispatcher Frame #7 (116ms)
        â”‚ scan.GetMaxPriority() â†’ -1 (Input)
        â”‚ âœ… Pick item #201 IMMEDIATELY    â”‚ Render: [], Input: [],
        â”‚    âš¡ Jumped over 194 MQTT items! â”‚ Background: [7-200]
â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
202-5K  â”‚ Post(MQTTÃ—4799, Background)      â”‚ Render: [], Input: [],
        â”‚                                  â”‚ Background: [7-5000]
â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        â”‚ ğŸ” Dispatcher Frame #8 (133ms)
        â”‚ scan.GetMaxPriority() â†’ -2 (Background)
        â”‚ âœ… Pick item #7 from Background  â”‚ Render: [], Input: [],
        â”‚                                  â”‚ Background: [8-5000]
â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
5001ï¸âƒ£   â”‚ Post(Click, Input) âš¡ HIGH PRIORITYâ”‚ Render: [], Input: [5001],
        â”‚                                  â”‚ Background: [8-5000]
â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        â”‚ ğŸ” Dispatcher Frame #9 (150ms)
        â”‚ scan.GetMaxPriority() â†’ -1 (Input)
        â”‚ âœ… Pick item #5001 IMMEDIATELY   â”‚ Render: [], Input: [],
        â”‚    âš¡ Jumped over 4993 MQTT items!â”‚ Background: [8-5000]
```

**Key findings:**

1. **No scanning needed**: `GetMaxPriority()` is O(1) - looks at max key in SortedList
2. **Priorities in separate buckets**: Render, Input, and Background are different chains
3. **High priority always wins**: Click #201 and #5001 picked immediately, jumping over thousands of MQTT items
4. **Background processing interrupted**: When clicks arrive, input starvation prevention stops background processing
5. **Queued items persist**: Even though clicks get through, background items remain queued consuming memory

**Why MQTT processing is throttled:**
```
Scenario: 4,993 MQTT messages queued at Background priority (-2)

Frame 1 (0-16ms):    Process ~5 background items
                     User moves mouse â†’ HasPendingInput = true
                     Stop processing background items âŒ

Frame 2 (16-33ms):   User still moving mouse
                     Skip background processing â­ï¸

Frame 3 (33-50ms):   No pending input
                     Process ~5 more background items
                     Time check: < 1 second since last input
                     Continue... but user clicks mouse âŒ
                     
Result:              Background items get processed in short bursts
                     between user interactions, not continuously.
                     At 60 FPS with frequent input, may process
                     only 30-60 background items/second (not 8,000!)
```

**Timeline: How items are queued and picked sequentially**

This diagram shows the actual timeline of items arriving vs. being picked and processed:

```mermaid
gantt
    title Queue Timeline: Items Arriving (ğŸ“¥) vs. Picked/Processed (âœ…)
    dateFormat X
    axisFormat %Lms
    
    section ğŸ“¥ Items Queued (Input Priority)
    Msg #1       :done, q1, 0, 1
    Msg #2       :done, q2, 0, 1
    Msg #3       :done, q3, 0, 1
    Msg #4-8     :done, q4, 1, 1
    Msg #9-16    :done, q5, 2, 2
    Msg #17-40   :done, q6, 4, 6
    Msg #41-80   :done, q7, 10, 10
    Msg #81-160  :done, q8, 20, 20
    Msg #161-320 :done, q9, 40, 40
    Msg #321-800 :done, q10, 80, 100
    Msg #801-1600:done, q11, 180, 200
    (continues...)  :done, q12, 380, 620
    
    section âœ… Items Picked & Processed (60 FPS = 16.6ms/frame)
    Pick #1      :crit, p1, 16, 1
    Pick #2      :crit, p2, 33, 1
    Pick #3      :crit, p3, 50, 1
    Pick #4-6    :crit, p4, 66, 17
    Pick #7-9    :crit, p5, 100, 17
    Pick #10-12  :crit, p6, 133, 17
    Pick #13-15  :crit, p7, 166, 17
    Pick #16-18  :crit, p8, 200, 17
    Pick #19-36  :crit, p9, 250, 117
    Pick #37-60  :crit, p10, 400, 200
    
    section ğŸ“Š Queue Size Growth
    Queue: 0     :milestone, m1, 0, 0
    Queue: 6     :milestone, m2, 16, 0
    Queue: 150   :milestone, m3, 100, 0
    Queue: 794   :milestone, m4, 180, 0
    Queue: 3970  :milestone, m5, 500, 0
    Queue: 7940  :milestone, m6, 1000, 0
```

**How the dispatcher picks items (with input starvation prevention):**

```
Timeline â”‚ Events
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  0ms    â”‚ ğŸ“¥ Msg #1 â†’ Background Queue [Queue: 1]
  0ms    â”‚ ğŸ“¥ Msg #2 â†’ Background Queue [Queue: 2]
  0ms    â”‚ ğŸ“¥ Msg #3 â†’ Background Queue [Queue: 3]
  ...    â”‚ ğŸ“¥ (messages keep arriving every 0.125ms)
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  16ms   â”‚ ğŸ” ExecuteJobsCore: Background priority, check input
         â”‚ âœ… No pending input, pick Msg #1 [Queue: 128 â†’ 127]
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  20ms   â”‚ ğŸ–±ï¸ User moves mouse (HasPendingInput = true)
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  33ms   â”‚ ğŸ” ExecuteJobsCore: Background priority, check input
         â”‚ âŒ Pending input detected! Stop processing background items
         â”‚ ğŸ’¤ Yield to next frame [Queue: 264 (accumulated during pause)]
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  50ms   â”‚ ğŸ” ExecuteJobsCore: Background priority, check input
         â”‚ âŒ Still pending input! Stop again
         â”‚ ğŸ’¤ Yield [Queue: 400]
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  66ms   â”‚ ğŸ–±ï¸ Mouse input processed (HasPendingInput = false)
         â”‚ ğŸ” ExecuteJobsCore: Check input
         â”‚ âœ… No pending input, pick Msg #2-4 [Queue: 528 â†’ 525]
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  83ms   â”‚ ğŸ” ExecuteJobsCore: Check time since last input
         â”‚ â±ï¸ Only 17ms since last input (< 1 second)
         â”‚ âœ… Pick Msg #5-7 [Queue: 664 â†’ 661]
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  100ms  â”‚ ğŸ” ExecuteJobsCore: Process batch
         â”‚ âœ… Pick Msg #8-10 [Queue: 800 â†’ 797]
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  200ms  â”‚ ğŸ–±ï¸ User clicks (HasPendingInput = true again)
         â”‚ âŒ Stop processing background! [Queue: 1,600]
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  1s     â”‚ ğŸ“Š CHECKPOINT: 
         â”‚    8,000 messages arrived
         â”‚    ~120 processed (interrupted by input multiple times)
         â”‚    7,880 queued (GROWING!)
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  5s     â”‚ ğŸ“Š CHECKPOINT: 
         â”‚    40,000 messages arrived
         â”‚    ~600 processed (frequent input interruptions)
         â”‚    39,400 queued (CRITICAL!)
         â”‚ âš ï¸ Dispatcher processed items only during input-free windows
```

**The compounding problem:**

```
Without input starvation prevention (theoretical):
  Processing: 60 FPS Ã— 5 items/frame = 300 items/second
  At 8K arrival: Queue grows by 7,700 items/second
  
With input starvation prevention (actual):
  Processing: Interrupted every 50-200ms by input checks
              Only ~30-60 items/second during active use
  At 8K arrival: Queue grows by 7,940-7,970 items/second
  
Result: Input starvation prevention makes MQTT queue growth
        WORSE while keeping UI responsive to user input.
```

**The key problem:** Items arrive at 0.125ms intervals, but the dispatcher can only process a limited number per frame:

```
Arrival Rate:    8 items/ms  (8,000/second)
Pick Rate:       Variable - depends on:
                 â€¢ Frame budget (16.6ms @ 60 FPS)
                 â€¢ Other UI work (rendering, layout, input)
                 â€¢ Input starvation prevention kicking in
Accumulation:    When arrival >> processing, queue grows
Result:          Queue can explode from 0 â†’ 40K in seconds
```

**Why input starvation prevention makes it worse:**

If you post MQTT updates at Background priority (-2), the dispatcher will:
1. Process a few background items
2. Check for pending input or time limit
3. **Stop processing background items** if either condition met
4. Wait until next frame to try again

This means background items may only get 10-100ms of processing time per second, not continuous processing. High-priority items (Render, Normal) continue unaffected.

**The Problem:**

- **Incoming rate**: 8,000 items/second = 8 items/millisecond
- **Processing rate**: 60 FPS Ã— 3 items/frame = ~180 items/second
- **Net accumulation**: 7,820 items/second pile up
- **Time to freeze**: Queue grows to 40K items in just 5 seconds

At 8K messages/second with zero delay, the queue can accumulate thousands of pending
items in seconds, causing:

- Memory pressure from queued delegates and captured state.
- GC pressure as the large object heap grows.
- UI freezes when the queue backlog finally processes.

### Why low priority doesn't help (and makes it worse)

You might think scheduling work at `DispatcherPriority.Background` (-2) or lower avoids problems. **It makes them worse** because:

1. **Still allocates memory**: Each work item creates a `DispatcherOperation` wrapper, captures closures, and consumes heap space.
2. **Slower processing**: Input starvation prevention limits background processing to brief windows (10-100ms bursts).
3. **No automatic drops**: Items accumulate until manually cleared or the app crashes.
4. **Hidden backlog**: Unlike higher priorities, background items can sit queued for minutes while the UI appears responsive.

**Measured impact** (from actual source):
- Background items only process when `job.Priority > DispatcherPriority.Input` check fails
- If user is moving mouse or typing, background processing pauses frequently
- Default 1-second timeout means background work gets at most 1 second of continuous execution before yielding

A queue with 50,000 low-priority items still consumes 10+ MB of memory and won't clear for minutes when the user is actively interacting with the UI.

**Implementation Details:**

The Avalonia dispatcher uses a `DispatcherPriorityQueue` internally with sophisticated execution control:

**Priority Queue (O(1) operations):**
- Uses `SortedList<int, PriorityChain>` to organize work by priority
- `GetMaxPriority()`: Returns highest priority in O(1) by checking `Keys[count-1]`
- `Peek()`: Returns next job in O(1) by accessing `Values[count-1].Head`
- Each priority level maintains a linked list (chain) of operations

**ExecuteJobsCore Processing Logic:**
```csharp
// Simplified from actual Dispatcher.Queue.cs
while (true) {
    job = _queue.Peek();  // O(1) - get highest priority job
    if (job == null) break;
    
    // Input starvation prevention
    if (job.Priority <= DispatcherPriority.Input) {
        if (HasPendingInput() || TimeSinceLastInput() > 1 second) {
            break;  // Stop processing, defer to next frame
        }
    }
    
    // Execute the job
    _queue.Dequeue();
    job.Execute();
}
```

**Memory overhead per queued item:**
- `DispatcherOperation` wrapper: ~120 bytes
- Delegate closure: Variable (depends on captured variables)
- Chain linkage: 2 Ã— pointer size (16 bytes on 64-bit)
- Total minimum: ~150 bytes per item

**Input Responsiveness:**
- The dispatcher checks `IDispatcherImplWithPendingInput.HasPendingInput` before processing items at or below Input priority
- On platforms without this interface, falls back to time-based checking only
- `MaximumInputStarvationTime` configurable via `DispatcherOptions.InputStarvationTimeout` (default: 1 second)

The core queue operations are fast (O(1)), but all items consume memory until processed, and low-priority items face frequent interruptions from input starvation prevention.

**Priority Order** (highest to lowest, with numeric values):

- `Send` (9) - Synchronous operations (highest)
- `Normal` (8) - Standard priority for most operations
- `DataBind` (7) - Data binding operations (obsolete, WPF compatibility)
- `AsyncRenderTargetResize` (6) - Platform-specific render target resizing
- `BeforeRender` (5) - Pre-render operations
- `Render` (4) - **Rendering and layout** â­
- `AfterRender` (3) - Post-render synchronization
- `UiThreadRender` (2) - Platform render timer
- `Loaded` (1) - Post-render initialization
- `Default` (0) - **Baseline foreground priority**
- `Input` (-1) - **User input events** (lower than render!)
- `Background` (-2) - **Background operations**
- `ContextIdle` (-3) - Context idle operations
- `ApplicationIdle` (-4) - Application idle
- `SystemIdle` (-5) - System idle (lowest)

References:

- Avalonia Dispatcher source: https://github.com/AvaloniaUI/Avalonia/tree/master/src/Avalonia.Base/Threading
- WPF Dispatcher (similar model): https://learn.microsoft.com/en-us/dotnet/desktop/wpf/advanced/threading-model

**Verification - All Source Files Fetched from Avalonia Repository (Feb 9, 2026):**

ALL 22 files from the Threading directory were fetched and analyzed to verify accuracy. You can click any link below to view the actual source code:

**Core Dispatcher Files:**
1. **Dispatcher.cs** - Main dispatcher class  
   https://raw.githubusercontent.com/AvaloniaUI/Avalonia/master/src/Avalonia.Base/Threading/Dispatcher.cs
2. **Dispatcher.Queue.cs** - ExecuteJobsCore main loop and queue processing  
   https://raw.githubusercontent.com/AvaloniaUI/Avalonia/master/src/Avalonia.Base/Threading/Dispatcher.Queue.cs  
   (Confirmed: `job = _queue.Peek()` and input starvation prevention)
3. **Dispatcher.Invoke.cs** - Invoke/InvokeAsync methods  
   https://raw.githubusercontent.com/AvaloniaUI/Avalonia/master/src/Avalonia.Base/Threading/Dispatcher.Invoke.cs
4. **Dispatcher.MainLoop.cs** - PushFrame and main loop management  
   https://raw.githubusercontent.com/AvaloniaUI/Avalonia/master/src/Avalonia.Base/Threading/Dispatcher.MainLoop.cs
5. **Dispatcher.Timers.cs** - Timer scheduling and promotion  
   https://raw.githubusercontent.com/AvaloniaUI/Avalonia/master/src/Avalonia.Base/Threading/Dispatcher.Timers.cs
6. **Dispatcher.Exceptions.cs** - Exception handling filters  
   https://raw.githubusercontent.com/AvaloniaUI/Avalonia/master/src/Avalonia.Base/Threading/Dispatcher.Exceptions.cs

**Priority System Files:**
7. **DispatcherPriority.cs** - Priority enum definitions and arithmetic  
   https://raw.githubusercontent.com/AvaloniaUI/Avalonia/master/src/Avalonia.Base/Threading/DispatcherPriority.cs  
   (Confirmed: Render=4, Input=-1, Background=-2)
8. **DispatcherPriorityQueue.cs** - SortedList<int, PriorityChain> implementation  
   https://raw.githubusercontent.com/AvaloniaUI/Avalonia/master/src/Avalonia.Base/Threading/DispatcherPriorityQueue.cs  
   (Confirmed: Line 14: `private readonly SortedList<int, PriorityChain> _priorityChains;`)
9. **DispatcherPriorityAwaitable.cs** - Async/await priority support  
   https://raw.githubusercontent.com/AvaloniaUI/Avalonia/master/src/Avalonia.Base/Threading/DispatcherPriorityAwaitable.cs

**Operation Management:**
10. **DispatcherOperation.cs** - Operation wrapper and execution  
    https://raw.githubusercontent.com/AvaloniaUI/Avalonia/master/src/Avalonia.Base/Threading/DispatcherOperation.cs
11. **DispatcherFrame.cs** - Frame-based execution  
    https://raw.githubusercontent.com/AvaloniaUI/Avalonia/master/src/Avalonia.Base/Threading/DispatcherFrame.cs
12. **DispatcherTimer.cs** - Timer implementation  
    https://raw.githubusercontent.com/AvaloniaUI/Avalonia/master/src/Avalonia.Base/Threading/DispatcherTimer.cs
13. **DispatcherOptions.cs** - Configuration options  
    https://raw.githubusercontent.com/AvaloniaUI/Avalonia/master/src/Avalonia.Base/Threading/DispatcherOptions.cs

**Synchronization Context:**
14. **AvaloniaSynchronizationContext.cs** - Main synchronization context  
    https://raw.githubusercontent.com/AvaloniaUI/Avalonia/master/src/Avalonia.Base/Threading/AvaloniaSynchronizationContext.cs
15. **NonPumpingSyncContext.cs** - Non-pumping lock helper  
    https://raw.githubusercontent.com/AvaloniaUI/Avalonia/master/src/Avalonia.Base/Threading/NonPumpingSyncContext.cs
16. **CulturePreservingExecutionContext.cs** - Culture preservation for .NET Framework  
    https://raw.githubusercontent.com/AvaloniaUI/Avalonia/master/src/Avalonia.Base/Threading/CulturePreservingExecutionContext.cs

**Interfaces & Support:**
17. **IDispatcher.cs** - Public dispatcher interface  
    https://raw.githubusercontent.com/AvaloniaUI/Avalonia/master/src/Avalonia.Base/Threading/IDispatcher.cs
18. **IDispatcherImpl.cs** - Platform implementation interfaces  
    https://raw.githubusercontent.com/AvaloniaUI/Avalonia/master/src/Avalonia.Base/Threading/IDispatcherImpl.cs
19. **DispatcherEventArgs.cs** - Event argument base class  
    https://raw.githubusercontent.com/AvaloniaUI/Avalonia/master/src/Avalonia.Base/Threading/DispatcherEventArgs.cs
20. **DispatcherUnhandledExceptionEventArgs.cs** - Exception event args  
    https://raw.githubusercontent.com/AvaloniaUI/Avalonia/master/src/Avalonia.Base/Threading/DispatcherUnhandledExceptionEventArgs.cs
21. **DispatcherUnhandledExceptionFilterEventArgs.cs** - Exception filter event args  
    https://raw.githubusercontent.com/AvaloniaUI/Avalonia/master/src/Avalonia.Base/Threading/DispatcherUnhandledExceptionFilterEventArgs.cs
22. **ThreadSafeObjectPool.cs** - Object pooling utility  
    https://raw.githubusercontent.com/AvaloniaUI/Avalonia/master/src/Avalonia.Base/Threading/ThreadSafeObjectPool.cs

**Directory Listing:**  
https://github.com/AvaloniaUI/Avalonia/tree/master/src/Avalonia.Base/Threading

Every piece of information in this guide was cross-referenced against these 22 actual source files.

## ObservableCollection and data binding

Each item added to an `ObservableCollection` triggers:

1. `CollectionChanged` event.
2. Binding updates in all subscribed controls.
3. Potential layout/measure passes.
4. Possible virtualization recalculation.

At high throughput, these updates compound. Mitigations:

- Use virtualized controls (DataGrid, TreeView with virtualization).
- Use `SourceList<T>` from DynamicData with `Edit()` for batch operations.
- **Never call `SourceList.Add()` in a loop** â€” use `Edit(list => { ... })` instead.
  Each `Add()` emits a separate `IChangeSet` â†’ separate `Bind()` â†’ separate
  `CollectionChanged`. `Edit()` batches everything into one.
- Move ViewModel creation off the UI thread with `.Select(Map)` before `.Buffer()`.
- Limit visible item count (e.g., rolling window of last N messages).

References:

- DynamicData: https://github.com/reactivemarbles/DynamicData
- Avalonia Controls source: https://github.com/AvaloniaUI/Avalonia/tree/master/src/Avalonia.Controls

## Rendering and layout costs

Each UI update can trigger:

- Layout pass (measure/arrange).
- Render pass.
- Composition and GPU upload.

Strategies to reduce rendering cost:

- Avoid complex templates for high-frequency items.
- Use simple text/primitives instead of nested panels.
- Disable animations during high-throughput periods.
- Consider `RenderOptions.BitmapInterpolationMode` for image-heavy UIs.

## Virtualization

Virtualized controls only create UI elements for visible rows. This is critical
at scale â€” without virtualization, 50K items means 50K live UI elements consuming
memory and triggering layout passes.

### DataGrid (row virtualization â€” built-in)

Avalonia's `DataGrid` implements its own row virtualization via `DataGridDisplayData`,
which maintains `_recyclableRows` and `_fullyRecycledRows` stacks.
`DataGridRowsPresenter` extends `Panel` directly â€” it does **not** use
`VirtualizingStackPanel`. The virtualization is internal to the DataGrid control.

- Source: `DataGridRowsPresenter : Panel` (confirmed: zero references to
  `VirtualizingStackPanel` in the entire DataGrid repo).
- Row recycling: `DataGridDisplayData._recyclableRows` / `_fullyRecycledRows`.

No configuration needed â€” virtualization is always active.

### TreeDataGrid (hierarchical virtualization)

The Topic Explorer uses `HierarchicalTreeDataGridSource<T>` from
[Avalonia.Controls.TreeDataGrid](https://github.com/fidarit/TreeDataGrid.Avalonia)
(MIT-licensed FOSS fork, NuGet: `Avalonia.Controls.TreeDataGrid 11.3.0`).

TreeDataGrid virtualizes **all** levels of the hierarchy (root + nested), unlike
the standard `TreeView` which uses a plain `StackPanel` and cannot virtualize.

> **Note:** `VirtualizingStackPanel` on `TreeView` (`TreeView.ItemsPanel`) is
> [not supported](https://github.com/AvaloniaUI/Avalonia/issues/10985) in
> Avalonia 11.x. The panel only virtualizes root-level items; each
> `TreeViewItem` has its own nested `StackPanel` and Avalonia lacks the
> `VirtualizingStackPanel.IsVirtualizing` attached property from WPF.

**Key implementation details:**

- `HierarchicalTreeDataGridSource` observes the bound
  `ReadOnlyObservableCollection` via `INotifyCollectionChanged` â€” no need to
  rebuild the source when items change.
- Selection is managed via `source.RowSelection!.SelectionChanged` event
  (named handler + explicit unsubscribe to prevent memory leaks).
- Expansion state is preserved across item additions because the source is
  stable (not rebuilt on every change).

### ListBox (virtualization by default)

Avalonia `ListBox` uses `VirtualizingStackPanel` out of the box. No special
config needed.

### TreeView (NOT virtualized)

The standard `TreeView` uses `StackPanel` (not `VirtualizingStackPanel`).
For large trees, use `TreeDataGrid` instead.

## Recommended settings

| Scenario | Bounded | Capacity | Full Mode | Msg Delay | Pkt Delay | Msg Buffer | Pkt Buffer | Counter Update |
|----------|---------|----------|-----------|-----------|-----------|------------|------------|----------------|
| 1K msg/s | Optional | 5,000 | Drop newest | 0-2 ms | 0-2 ms | 50 ms | 50 ms | 200 ms |
| 5K msg/s | Yes | 20,000 | Drop newest | 2-5 ms | 2-5 ms | 150 ms | 150 ms | 200 ms |
| 8K+ msg/s | Yes | 50,000 | Drop oldest/write | 5-10 ms | 5-10 ms | 300 ms | 300 ms | 200 ms |

### Throughput profiles

The app ships with built-in **throughput profiles** (Low, Medium, High, Custom)
that apply all of the above settings in one click. Each profile sets channel
configuration, consumer delays, reactive-stream buffer windows, UI list limits,
and counter refresh rate.

| Setting | Low (â‰¤ 1K) | Medium (â‰¤ 5K) | High (8K+) | Custom |
|---------|-----------|--------------|-----------|--------|
| Bounded channel | No | Yes | Yes | Yes |
| Capacity | 5,000 | 20,000 | 50,000 | 50,000 |
| Full mode | Drop newest | Drop newest | Drop newest | Drop newest |
| Message delay (ms) | 0 | 3 | 5 | 5 |
| Packet delay (ms) | 0 | 3 | 5 | 5 |
| Message buffer (ms) | 50 | 150 | 300 | 200 |
| Packet buffer (ms) | 50 | 150 | 300 | 500 |
| Max UI items | 150,000 | 150,000 | 150,000 | 150,000 |
| Trim batch size | 500 | 500 | 500 | 500 |
| Counter update (ms) | 200 | 200 | 200 | 200 |

When **Custom** is selected, all controls are editable and you can experiment freely.
With any preset profile the controls are read-only so the tuned values stay
consistent.

### Buffer time and UI thread pressure

The `.Buffer(N ms)` window controls how often the UI thread wakes up to process a
batch of pre-built ViewModels:

| Buffer window | Batches/sec | UI thread impact |
|---------------|-------------|------------------|
| **50 ms** (Low profile) | ~20 | Highest â€” the UI thread runs `Edit()` 20Ã— per second. Acceptable at low message rates because each batch is small. |
| **150 ms** (Medium profile) | ~7 | Moderate â€” fewer, larger batches strike a balance between responsiveness and throughput. |
| **300 ms** (High profile) | ~3 | Lowest â€” the UI thread only runs `Edit()` ~3Ã— per second, ideal when thousands of items arrive per batch. |

**Rule of thumb:** the more relaxed the profile (lower throughput), the shorter the
buffer window can be, because fewer items arrive per batch and each `Edit()` call is
cheap. At low rates you can afford to wake the UI thread more often, which gives
better perceived responsiveness (items appear almost instantly).

Conversely, at high throughput a short buffer window would flood the UI thread with
large batches 20Ã— per second, causing layout/render storms and sluggish input.
A longer buffer window collects more items per batch but fires far less frequently,
keeping the UI thread free for user interaction.

```
 Low throughput    â”€â”€â–¶  short buffer (50 ms)   â”€â”€â–¶  many small batches  â”€â”€â–¶  responsive UI
 High throughput   â”€â”€â–¶  long buffer (300 ms)   â”€â”€â–¶  few large batches   â”€â”€â–¶  stable UI
```

Tips:

- Monitor the "Buffered" counter; if it keeps growing, increase delay or accept drops.
- If drops are unacceptable, use "Wait" mode but expect backpressure on MQTT processing.
- Consider disabling Topic Explorer recording during extreme bursts.

## Alternative architectures for extreme throughput

If the app must handle sustained 10K+ msg/s without drops:

1. **Sampling**: Only process every Nth message for UI display.
2. **Aggregation**: Batch messages and display summaries (count, rate, last value).
3. **Separate storage**: Write to disk/database and query for display.
4. **Headless mode**: Disable UI updates entirely during capture, review later.

These patterns trade real-time visibility for stability and are common in
high-frequency trading, telemetry, and IoT monitoring systems.

## Implementation in this app

**Why we don't use the dispatcher queue directly:**

As documented above, posting 8K messages/second directly to the dispatcher would:
- Accumulate 7,900 items/second even without any processing
- Consume ~1.2 MB/second just for operation wrappers
- Get interrupted by input starvation prevention every 50-200ms
- Take minutes to clear the backlog once MQTT traffic stops

**Solution: Channel â†’ Subject â†’ Select â†’ Buffer â†’ Edit pipeline**

The app uses a three-stage pipeline that completely avoids the dispatcher queue for
high-frequency data:

```
Stage 1: Channel (ingestion + backpressure)
    MQTTnet callback â†’ Channel<T>.Writer.WriteAsync/TryWrite()
    - Bounded capacity with configurable full mode (drop/wait)
    - Non-blocking writes for packet inspection (connection safety)
    - Counters track received/enqueued/dropped

Stage 2: Reactive streams (distribution)
    Consumer task (LongRunning) â†’ Subject<T>.OnNext()
    - Single dedicated thread per channel (message + packet)
    - Subject<T>.Publish().RefCount() for hot multicast observable
    - Session events notify subscribers when streams connect/disconnect
    - Counters track notified messages

Stage 3: Subscriber pipelines (transform + batch + UI insert)
    stream.Select(Map).Buffer(Nms).Where(>0).ObserveOn(UI).Subscribe(Edit)
    - .Select() creates ViewModels on the stream thread (not UI thread)
    - .Buffer() collects pre-built VMs into time-windowed batches
    - .ObserveOn() marshals to UI thread
    - SourceList.Edit() inserts entire batch as a single IChangeSet
    - Topic Explorer runs two-phase Select: tree walk + node creation on
      stream thread, Edit() flush + AddMessage on UI thread
```

```mermaid
sequenceDiagram
    participant MT as ğŸ§µ MQTT Thread
    participant CB as ğŸ“¦ Channel Buffer
    participant CT as ğŸ§µ Consumer Task<br/>(LongRunning)
    participant SB as ğŸ”´ Subject<T>
    participant RX as âš¡ Rx Pipeline<br/>(Selectâ†’Buffer)
    participant UT as ğŸ§µ UI Thread

    MT->>CB: Write msg (bounded/drop)
    Note over CB: ğŸ“Š Capacity + Full Mode<br/>Drop callback on overflow

    loop Single-reader consumer
        CT->>CB: WaitToReadAsync + TryRead
        CB-->>CT: eventArgs
        CT->>CT: _notifiedMessagesCount++
        CT->>SB: OnNext(eventArgs)
    end

    SB->>RX: .Select(Map) on consumer thread
    Note over RX: Create ViewModel here<br/>(off UI thread)

    RX->>RX: .Buffer(200ms) collects VMs
    RX->>UT: .ObserveOn(MainThread)

    Note over UT: SourceList.Edit()<br/>single IChangeSet<br/>â†’ 1 CollectionChanged<br/>â†’ 1 layout pass
```

**Topic Explorer two-phase pipeline:**

The Topic Explorer is more complex because it must walk a hierarchical tree
and create intermediate nodes. Its pipeline splits into two explicit phases:

```mermaid
sequenceDiagram
    participant CT as ğŸ§µ Consumer Thread
    participant P1 as ğŸ”§ PrepareBatchInserts<br/>(stream thread)
    participant UT as ğŸ§µ UI Thread
    participant P2 as âœ… FlushBatchInserts<br/>(UI thread)

    CT->>P1: .Select(PrepareBatchInserts)
    Note over P1: Group by first segment<br/>Walk trie per topic<br/>Create missing nodes<br/>Stage SourceList adds
    P1-->>UT: .ObserveOn(MainThread)
    UT->>P2: (PendingAdds, MessageUpdates)
    Note over P2: Edit() per SourceList<br/>node.AddMessage()<br/>node.Clear() if tracking latest
```

- **Phase 1 (stream thread):** `PrepareBatchInserts` groups messages by first
  topic segment for cache-friendly traversal, then calls `ResolveOrCreateLeaf`
  for each message. The resolver walks a **trie** â€” `_rootLookup` (a
  `ConcurrentDictionary` on the page VM) holds root-level nodes, and each node
  has its own `ChildLookup` dictionary containing only its direct children. For
  a topic like `sensors/temperature/cpu`, the walk is:
  `_rootLookup["sensors"]` â†’ `.ChildLookup["temperature"]` â†’
  `.ChildLookup["cpu"]`. Missing nodes are created on the fly and staged in a
  `pendingAdds` dictionary for deferred `Edit()` on the UI thread.
- **Phase 2 (UI thread):** `FlushBatchInserts` flushes all pending adds with a
  single `Edit()` per `SourceList` and applies message data to target nodes.

**Trie-based node lookup:**

Previous versions used a single flat `ConcurrentDictionary` keyed by partial
topic paths (e.g. `"a"`, `"a/b"`, `"a/b/c"`). With thousands of unique topics
this dictionary grew very large, making lookups slower due to hash collisions
and poor cache locality.

The trie approach distributes the lookup across the tree itself:

```
_rootLookup                          (ConcurrentDictionary â€” root segments only)
â”œâ”€â”€ "sensors"  â†’  Node
â”‚   â””â”€â”€ .ChildLookup                 (ConcurrentDictionary â€” direct children only)
â”‚       â”œâ”€â”€ "temperature"  â†’  Node
â”‚       â”‚   â””â”€â”€ .ChildLookup
â”‚       â”‚       â”œâ”€â”€ "cpu"     â†’  Node (leaf)
â”‚       â”‚       â””â”€â”€ "gpu"     â†’  Node (leaf)
â”‚       â””â”€â”€ "humidity"     â†’  Node (leaf)
â””â”€â”€ "actuators"  â†’  Node
    â””â”€â”€ .ChildLookup
        â””â”€â”€ "valve"  â†’  Node (leaf)
```

Each dictionary is small (only direct children of one parent), so:
- Hash collisions are rare, keeping lookups O(1) in practice.
- The working set fits in CPU cache during grouped traversal.
- Thread safety is maintained via `ConcurrentDictionary` at every level.

**Group-by-first-segment optimisation:**

Before walking the trie, `PrepareBatchInserts` groups the incoming batch by
first topic segment (`topic.Split('/')[0]`). All messages under `sensors/â€¦`
are processed together, then all under `actuators/â€¦`, and so on. This keeps
the root lookup and the first-level `ChildLookup` hot in cache for the entire
group, avoiding thrashing between unrelated subtrees.

### Threading model

The app creates two `Task.Factory.StartNew(..., TaskCreationOptions.LongRunning)`
consumer tasks per connection:

| Task | Channel | Subject | Single writer? |
|------|---------|---------|----------------|
| `ConsumeReceivedMessagesAsync` | Message channel | `_messageSubject` | Yes â€” dedicated LongRunning thread |
| `ConsumePacketInspectionsAsync` | Packet channel | `_packetSubject` | Yes â€” dedicated LongRunning thread |

The log stream (`_logSubject`) is fed directly from `MqttNetEventLogger.LogMessagePublished`
which MQTTnet invokes on whatever thread triggers the log. This is acceptable because
log events are low-frequency and `Subject<T>.OnNext()` is safe when calls don't overlap
(MQTTnet serializes its log calls internally).

Each consumer task's loop:
```csharp
while (await reader.WaitToReadAsync(ct).ConfigureAwait(false))
{
    while (reader.TryRead(out var eventArgs))
    {
        _notifiedMessagesCount++;           // single-writer: safe plain increment
        _messageSubject?.OnNext(eventArgs); // single-writer: only this task calls OnNext

        var delayMs = Volatile.Read(ref _messageProcessingDelayMs);
        if (delayMs > 0)
            await Task.Delay(delayMs, ct).ConfigureAwait(false);
    }
}
```

Because `Subject<T>.OnNext()` is called from exactly one thread:
- The downstream `.Select(Map)` runs on this same thread
- ViewModel creation (string formatting, allocation) happens here â€” off the UI thread
- Only after `.ObserveOn(RxApp.MainThreadScheduler)` does work reach the UI thread

### Counter single-writer guarantees

Each counter has exactly one writer, so plain `counter++` is safe (no Interlocked needed):

| Counter | Writer thread | Location |
|---------|---------------|----------|
| `_receivedMessagesCount` | MQTTnet receive loop | `OnApplicationMessageReceived` |
| `_enqueuedMessagesCount` | MQTTnet receive loop (same as above) | After channel write succeeds |
| `_notifiedMessagesCount` | Message consumer task (LongRunning) | `ConsumeReceivedMessagesAsync` loop |
| `_droppedMessagesCount` | Channel drop callback (invoked on writer's thread) | `OnMessageDropped` |

The UI reads these on a configurable `Observable.Interval` (default 200ms, set via
the **Counter update (ms)** profile setting). On 64-bit systems, `long` reads/writes
are hardware-atomic so there are no torn reads. Cache coherence ensures eventual visibility
within the polling interval.

### Why SourceList.Edit() not Add()

`SourceList.Add()` internally calls `source.Edit(list => list.Add(item))` â€” each call
creates a separate `IChangeSet`. A batch of 50 items would produce 50 changesets,
each flowing through `Bind()` to trigger `CollectionChanged` and potential layout passes.

`SourceList.Edit()` accumulates all changes in a single `IChangeSet`:

```csharp
// 50 items â†’ 1 changeset â†’ 1 CollectionChanged â†’ 1 layout pass
_itemsSource.Edit(list =>
{
    foreach (var vm in batch)
    {
        list.Add(vm);
        if (list.Count > 1000) list.RemoveAt(0);
    }
});
```

### Why .Select() before .Buffer()

Without `.Select()`, the Buffer collects raw event args, then the UI thread must
create ViewModels AND insert them:

```
// âŒ All work on UI thread
stream â†’ Buffer(raw) â†’ ObserveOn(UI) â†’ foreach: CreateVM() + list.Add()
                                         â†‘ string formatting, allocation, event wiring
```

With `.Select()` before `.Buffer()`, VM creation runs on the consumer task thread:

```
// âœ… Only insertion on UI thread
stream â†’ Select(CreateVM) â†’ Buffer(VMs) â†’ ObserveOn(UI) â†’ Edit(bulk insert)
           â†‘ stream thread                                   â†‘ UI thread
```

**Key design decisions:**
- **Channels** buffer messages outside the UI thread (not in the dispatcher queue)
- **Consumer tasks** (LongRunning) provide single-threaded, single-writer semantics
- **Subject<T>** multicasts to multiple subscribers without re-reading the channel
- **Select()** moves ViewModel creation to the stream thread
- **Buffer()** time-windows pre-built VMs into batches
- **Edit()** inserts the batch as a single `IChangeSet` (one UI update)
- **Bounded channels** prevent unbounded memory growth
- **Counters** track received/enqueued/notified/buffered/dropped

**Comparison: Dispatcher Queue vs. Channel + Reactive Pipeline**

| Aspect | Direct Dispatcher Queue | Channel â†’ Subject â†’ Select â†’ Buffer â†’ Edit |
|--------|-------------------------|---------------------------------------------|
| **Memory control** | âŒ Unbounded (grows until OOM) | âœ… Bounded capacity with drop policy |
| **VM creation thread** | âŒ UI thread | âœ… Consumer task thread (via Select) |
| **UI updates per batch** | âŒ N items = N updates | âœ… N items = 1 update (via Edit) |
| **Tree node creation** | âŒ UI thread builds tree | âœ… Stream thread walks trie (via PrepareBatchInserts) |
| **Processing rate** | âŒ 30-120 items/sec with input | âœ… Controllable via delays |
| **Backpressure** | âŒ None (items pile up) | âœ… Channel blocks or drops when full |
| **UI responsiveness** | âŒ Frozen when clearing backlog | âœ… Always responsive |
| **Observability** | âŒ Hidden internal queue size | âœ… Visible counters |
| **Recovery** | âŒ Takes minutes after burst | âœ… Immediate (channel bounded) |

This architecture allows the app to handle sustained 8K+ msg/s while maintaining UI
responsiveness and predictable memory usage. The UI thread only runs `SourceList.Edit()`
with pre-built ViewModels â€” all transformation work happens on consumer task threads.

## Tuning workflow

1. Start with defaults (bounded, 200K capacity, drop newest, 5ms delays).
2. Connect and observe counters under expected load.
3. If "Buffered" grows continuously, increase delays or reduce capacity.
4. If "Dropped" is unacceptable, increase capacity or switch to "Wait" mode.
5. If UI is sluggish, increase delays or disable Topic Explorer recording.
6. For extreme throughput, consider sampling or aggregation patterns.
7. Monitor the consumer task thread â€” if `Select(Map)` is slow (complex VM creation),
   the buffer interval may need increasing.

## Further reading

- System.Threading.Channels: https://learn.microsoft.com/en-us/dotnet/core/extensions/channels
- System.Reactive (Rx.NET): https://github.com/dotnet/reactive
- DynamicData: https://github.com/reactivemarbles/DynamicData
- Backpressure: https://www.reactivemanifesto.org/glossary#Back-Pressure
- Producer-consumer pattern: https://en.wikipedia.org/wiki/Producer%E2%80%93consumer_problem
